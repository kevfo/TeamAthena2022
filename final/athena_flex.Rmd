---
title: "Team Athena"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme:
      version: 4
      bootswatch: minty
runtime: shiny
---

Background and Rationale { .storyboard}
=====================================  

### Singapore ranks first in the world for diabetes-induced kidney failure

```{r}
library(ggplot2) ; library(flexdashboard)

df <- data.frame(country=c("Singapore", "Qatar", "HongKong"),
                 rate=c(67.5, 63.9, 52.9))

# Basic barplot
ggplot(data=df, aes(x=country, y=rate)) +
  geom_bar(stat="identity", aes(fill = country))+
  geom_text(aes(label=rate), vjust=1.6, color="white", size=3.5)+
  theme_minimal()
```

***

The figure on the left hand side shows the percentage of diabetic kidney failure patients out of all kidney failure cases). 

According to the **NKF** (**N**ational **K**idney **F**oundation), in Singapore, 5.7 patients on average are diagnosed on a daily bases, making this a critical public health challenge for the society. 

Despite the healthcare aspects, a large population with kidney failure is also a huge financial burden for the country, in which $190 million is spent annually on dialysis treatment. 

Moreover, up to 1 million diabetic patients is predicted to be diagnosed in 2050. 

### Rationale

```{r}
knitr::include_graphics('images/ds.jpg')
```

***

We proposed a random forest model that serves as a prototypical tool for our target users - laboratory diagnosticians/doctors - to decide whether a person is suffering from renal failure as well as the stage of his/her illness. When the professional inputs the required values in accessing health outcomes of a person, the result can be displayed on the screen immediately. As prevention is key for curing diabetic renal failure, our App is created based on a large dataset. Based on National Registry of Diseases Office, approximately 30% of patients with diabetes are unaware of their poor kidney health condition. Having over 0.4 million diabetes patients in the country, if our App is in use, it is estimated that 130 thousand Singaporean can benefit from our application by identifying the disease at an earlier stage. 



Data Cleaning { .storyboard}
=====================================  

### There were many missing pieces of data scattered throughout the dataset.

```{r}
library(tidyverse)
data <- read_csv("kidney_disease.csv") %>% select(-c(id))

mData <- colSums(is.na(data)) %>% as.data.frame() %>% rename("count" = '.')
mData <- cbind(names(data), mData) %>% rename('var' = 'names(data)') %>% arrange(count)

ggplot(mData, aes(y = count, x = var)) + geom_col(aes(fill = var), show.legend = F, width = 0.5) + geom_text(aes(label = count)) +
  labs(x = 'Variable', y = 'Missing Data Count', title = "Amount of Missing Data by Variable")
```

***

Prior to using the data provided to perform exploratory data analysis /
train any models, it's probably wise to do *something* regarding the data on the left!

We have a lot of missing data (especially the `rbc`, the `rc`, and the `wc` column).  Since at least 25% of the data is missing in these variables, it would likely be better if these variables are left out altogether.  

Furthermore, we also have missing `age` data.  Since `age` appeared to be independent of most variables (i.e., not correlated in any way), it would also likely be safer to remove entries whose age are missing (i.e., one cannot predict another's age solely using the data provided).

### Using `mi` for model-based imputations

```{r}
knitr::include_graphics("images/oldie.PNG")
```

***

We note that it would be unwise to remove data as there are only 400 data points in the entire dataset.  Nevertheless, we also realized that most data could not be imputed in more "conventional" ways (e.g., hot / cold-deck imputation and median / mean imputation).

The latter was especially the case for categorical variables.  Hence, we decided to try and use the `mi` package from CRAN to impute missing values via chained regression analysis (for numerical data) and thereafter - using these results to predict the class of a data point (for categorical data).

The heatmap to the left shows where missing data is present (i.e., the dark spots)

### Re-visiting the image map after imputations

```{r}
knitr::include_graphics('images/oldandnewheat.PNG')
```

***

Based off trial and error, we ultimately decided to calculate 80 iterations (i.e., calculations) for each regression chain.  While some imputed values' means were somewhat different in each chain, we found that 80 iterations gave the best result (i.e., only one variable had highly variable means within each chain).

### Further examining the imputed values - how does the distribution of imputed data look like relative to the original, non-missing data?

```{r}
knitr::include_graphics("images/samplePlot.PNG")
```

***

`mi` also generates several plots for each variable that has missing data.  Shown is an example - the histogram to the left implies that the distribution of data for both observed, imputed, and completed data against values predicted by `mi`'s regression chain (i.e., the red line).

The middle graphs show that the actual values of the data points predicted by `mi` are similar to non-missing data points.

The rightmost graphs are residual plots - possibly showing that the values predicted by `mi` are still within reason.


Exploratory Data Analysis { .storyboard}
=====================================

### How does the distribution of data differ within numerical variables based on health status?

```{r}
knitr::include_graphics('images/boxplot.png')
```

***

Noticeably, those who are diseased have outlier data points compared to healthy individuals.  However, there appears to be sizable differences between the median values of diseased and healthy patients (at least for certain variables like specific gravity).

Yet, although we can still somewhat make out what variables we could use during model training (e.g., perhaps specific gravity due to the marked differences in values), the boxplot to the left is also difficult to interpret for purposes of feature selection.

### Are some variables more correlated with one another?

```{r}
knitr::include_graphics("images/trianglecor.png")
```

***

Based on the correlation plot itself, we note that several variable pairs are highly correlated.

Hence, moving forward, we should be mindful of these variables lest we end up using them in the same model.  This is because the variable importance of one variable might be significantly reduced if there is already another highly-correlated variable.  More importantly, multicolinearity may lead to the assumption that one variable is important and others not (when the truth may could be the aforementioned).

### Might a multivariate analysis of continuous variables reveal any individual dependencies?

```{r}
knitr::include_graphics("images/multiana.PNG")
```

***

We wanted to know if variables such as age, random blood glucose level, and potassium and sodium levels had individual dependencies so that we can further understand relationships that may exist within data.  

In the scatterplot correlogram, data highlighted in red are for diseased individuals while those in teal are for healthy individuals. We can observe that age is indeed not correlated with the other variables in any way. Sodium and potassium levels are also do no correlate that well with blood pressure.

Nonetheless, there seems to be marked differences between diseased and healthy individuals for the haemoglobin, specific gravity and packed cell volume variables.


### How do co-morbidies present themselves in healthy and diseased individuals?

```{r}
knitr::include_graphics("images/comorbid.PNG")
```

***

We see that healthy individuals are in good health - good appetite with none of the mentioned co-morbidies.

We can exploit this feature knowing that no person with kidney disease is also free from hypertension or some other co-morbidity!

### Possible feature engineering too using age ranges?

```{r}
knitr::include_graphics("images/agerange.PNG")
```

***

Out of curiosity, we also enginnered a new feature `agerange` that we thought could be useful when model training.  We have three categories of ages defined as follows (based off commonly accepted standards in the US as none could be found in the Singaporean context):

1.  **Elderly** => $\le$ 65
1.  **Middle age** => $\le$ 45, $\le$ 65
1.  **Young** => $\le$ 45

The amount of healthy individuals appear to increase with decreasing age - could we then use this as a feature in a classifier (i.e., lower age = higher probability of healthiness)?



Model Building { .storyboard}
=====================================

### Creating testing and training sets of data

```{r}
source("model.R")
setwd("C:/Users/Kevin/Desktop/Projects/R/TeamAthena2022/final")
knitr::include_graphics("images/training.PNG")
```

***

From observation, we noted that our data had 60% diseased patients and 40% healthy patients (i.e., the distribution is skewed).  Hence, via trial and error, we wanted to find a training / testing split that allowed for a similar proportion of diseased to healthy patients lest our models become biased towards one class.

The `caret` package was used for this portion and the next.

### Attempting cross-validation to improve the accuracy of our model

```{r}
knitr::include_graphics('images/cv.jpeg')
```

***

In order to maximize accuracy, we also perform cross-validation with 10 folds.

Like previously mentioned, this portion of the workflow was also done up in the `caret` package.

### Constructing an initial random forest model and evaluating its performance

```{r}
confusionMatrix(testing$classification, rfPredictions)
```

***

A random forest was fitted using `caret` before a confusion matrix (see left) was generated.  Our model - using *all* predictors - had very high accuracy and sensitivity.  

However, we identify two issues:

1.  Since this model performs so well, how do other models (e.g., logistic regression) compare to this and do they perform similarly well?  

1.  The variables used to predict kidney disease status are not all used in Singapore.

### Re-training our random forest model based on variables used in the Singaporean context

```{r}
knitr::include_graphics('images/variablestofocus.jpeg')
```

***

After some research, we found that the following variables are used for passive or active surveillance (green and yellow highlights respectively).  Because of this, we thought it would be wise to re-train our initial random forest model based on these variables.

A newly-constructed confusion matrix shows that our new model accuracy is 0.9742 with a sensitivity of 1.  So, we still have similarly high accuracy and in this case, perfect sensitivity.  However, we thought that the model could still be improved in terms of its practicality.

It appears unlikely that a user using this model would have both measures for passive or active surveillance for a patient.  So, it appears wise to narrow down the variables used in the random forest model.

### Cumulative variable importance plot to decide variables to use and retraining our model.

```{r}
knitr::include_graphics('images/cumImp.PNG')
```

***

Variable importance is a score that quantifies how "useful" a variable is in constructing a decision tree (or in this case, a random forest).  Looking at the plot, it appears that the increase in variable importance appears to not increase much at around 90% of the total variable importance.  

Upon looking at the confusion matrix, we see that this new model has a slight dip in accuracy (96.13%), but is still perfectly sensitive (we also have 6 false positives).  However, we think that the tradeoff between accuracy and lesser features is worthwhile - the features used are `sg`, `sc`, `htn`, `sod`, `bu`, `bgr`, and `al`.

While some variables are from passive and active surveillances, we still think that this model is useful as it reduces the workload for the user and produces rapid results.

### Benchmarking 

```{r}
knitr::include_graphics('images/questionMan.PNG')
```

***

We constructed the following models and found the following statistics:

1.  **Decision tree (via CART algorithm)**
    
    96.77% accurate, 94.79% sensitive.  Performance-wise, it is similar to our random forests model.  However, if the data is altered in any way, the decision tree's predictions may vary and the tree structure be different.
   
1.  **Logistic regression**

    Fortunately for this case, multicolinearity does not seem to be an issue as observed during exploratory data analysis.  It is 95.48 % accurate and 92.71% sensitive.

1.  **kNN** 

    Here, we used all numerical variables to construct the model - `k = 9` was the final `k` value chosen by `caret` during training.  Nevertheless, it is 82.58% accurate and 77.08% sensitive.
    
    Based on the exploratory data analysis, this already seems like a questionable idea at best - many data points are clustered together.

Applications { .storyboard}
====================================

### A trivial application to predict patient(s)' kidney health status.

```{r}
knitr::include_graphics('images/app.PNG')
```

***

- Prototype for predicting whether a patient has kidney disease or not
- As the target audience for the prototype is for lab diagnosticians or doctors who generally do not have too much computing knowledge, we chose a  user-friendly interface (i.e., to the point).
- There are 2 different interfaces- quick check & multiple data

*Quick Check*

- Allows user to input data for various parameters such as Creatinine Level, Albumin Level & whether the patient has hypertension.
- Data is then read to a dataframe & random forest model is applied
- Outputs a prediction on whether the patient has kidney disease or not

*Multiple Data*

- User can enter 1 or more datasets as a .csv file type
- Data uploaded will be displayed as a table
- Data is then read to a dataframe & random forest model is applied
- Outputs a prediction on whether the patient(s) has / have kidney disease or not
- Validation to check whether file upload is of the correct format

**Cons**

- Unable to deal with missing data, so user must manually remove missing data rows
    - Can use cold deck imputation to insert values for missing data using data from another external source as data that the random forest model was trained on was limited

### A RESTful API service to share our work with healthcare institutions / application developers.

```{r}
setwd("C:/Users/Kevin/Desktop/Projects/R/TeamAthena2022/final")
knitr::include_graphics("images/API.PNG")
```

***

1) Made using the `plumber` package (in R) and fitted with the Swagger UI
    - Can share code with others in a simple manner
    - Particularly useful for healthcare settings, systems, or even application developers
    - Based on the REST (i.e., **Re**presentational **s**tate **t**ransfer) architecture
    
1) Two endpoints:
    - `/health-check` - a `GET` request that (for the most part) returns a `200 OK` response in order to verify that the API is indeed up and running. 
    - `/predict` - a `POST` request that accepts JSON and returns a JSON payload with the predicted status of the patient(s) in question.



